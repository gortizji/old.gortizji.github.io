[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a PhD student at EPFL working under the supervision of Pascal Frossard. My research focuses on understanding deep learning with the aim to design better and more reliable neural networks that exploit prior knowledge about the world.\nLast summer, I was a research intern at Google Research in Zurich, where I worked with Efi Kokiopoulou and Rodolphe Jenatton. In spring 2022, I visited Philip Torr\u0026rsquo;s lab at the University of Oxford as part of the ELLIS PhD Program, where I conducted research on the robustness of neural networks.\nBefore starting my PhD, I lived in The Netherlands for two years, where I worked on sampling theory for tensors and graph data at TU Delft. In the past, I also spent some great time at Philips Research in Hamburg working on self-supervised deep learning for medical imaging. Some time ago, I was an undergraduate student at Universidad Politecnica de Madrid, Spain.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://gortizji.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a PhD student at EPFL working under the supervision of Pascal Frossard. My research focuses on understanding deep learning with the aim to design better and more reliable neural networks that exploit prior knowledge about the world.\nLast summer, I was a research intern at Google Research in Zurich, where I worked with Efi Kokiopoulou and Rodolphe Jenatton. In spring 2022, I visited Philip Torr\u0026rsquo;s lab at the University of Oxford as part of the ELLIS PhD Program, where I conducted research on the robustness of neural networks.","tags":null,"title":"Guillermo Ortiz-Jiménez","type":"authors"},{"authors":["G. Ortiz-Jimenez*","P. de Jorge*","A. Sanyal","A. Bibi","P. Dokania","P. Frossard","G. Rogez","P. Torr"],"categories":[],"content":"","date":1656325364,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656325364,"objectID":"2387ab5d9c317e1f80272853d5f48ea6","permalink":"https://gortizji.github.io/publication/co_oxford/","publishdate":"2022-06-27T12:20:44+02:00","relpermalink":"/publication/co_oxford/","section":"publication","summary":"We find that the interplay between the structure of the data and the dynamics of AT plays a fundamental role in CO. Specifically, through active interventions on typical datasets of natural images, we establish a causal link between the structure of the data and the onset of CO in single-step AT methods.","tags":[],"title":"Catastrophic overfitting is a bug but also a feature","type":"publication"},{"authors":["A. Modas*","R. Rade*","G. Ortiz-Jimenez","S.M. Moosavi-Dezfooli","P. Frossard"],"categories":[],"content":"","date":1656325364,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656325364,"objectID":"e4483ff10d8fccc5b415a0e1240825d6","permalink":"https://gortizji.github.io/publication/prime_2021/","publishdate":"2021-12-27T12:20:44+02:00","relpermalink":"/publication/prime_2021/","section":"publication","summary":"We propose PRIME, a general data augmentation scheme that consists of simple families of max-entropy image transformations. We show that PRIME outperforms the prior art for corruption robustness, while its simplicity and plug-and-play nature enables it to be combined with other methods to further boost their robustness.","tags":[],"title":"PRIME: A few primitives can boost robustness to common corruptions","type":"publication"},{"authors":["J. Maroto","G. Ortiz-Jimenez","P. Frossard"],"categories":[],"content":"","date":1649847149,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649847149,"objectID":"e24fb7ba9d0abd622cd3cb16a2205a1f","permalink":"https://gortizji.github.io/publication/knowledge_distillation/","publishdate":"2022-04-13T12:52:29+02:00","relpermalink":"/publication/knowledge_distillation/","section":"publication","summary":"We present a thorough analysis and provide general guidelines to distill knowledge from a robust teacher and boost the clean and adversarial performance of a student model even further.","tags":[],"title":"On the benefits of knowledge distillation for adversarial robustness","type":"publication"},{"authors":["G. Yüce*","G. Ortiz-Jimenez*","B. Besbinar","P. Frossard"],"categories":[],"content":"","date":1638787949,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638787949,"objectID":"a3dc047ed7639e830bc921aa42c364bd","permalink":"https://gortizji.github.io/publication/inr_dictionaries/","publishdate":"2021-12-06T12:52:29+02:00","relpermalink":"/publication/inr_dictionaries/","section":"publication","summary":"We show that most INR families are analogous to structured signal dictionaries whose atoms are integer harmonics of the set of initial mapping frequencies whose inductive bias is akin to that of signal dictionary formed by the eigenfunctions of the NTK at initialization. We reveal that meta-learning has a reshaping effect of the NTK analogous to dictionary learning, building dictionary atoms as a combination of the examples seen during meta-training.","tags":[],"title":"A structured dictionary perspective on implicit neural representations","type":"publication"},{"authors":["G. Ortiz-Jimenez","S.M. Moosavi-Dezfooli","P. Frossard"],"categories":[],"content":"","date":1632824564,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632824564,"objectID":"2149b760f6891397b6b45558210827f9","permalink":"https://gortizji.github.io/publication/linearized_nets_neurips_2021/","publishdate":"2021-06-12T12:20:44+02:00","relpermalink":"/publication/linearized_nets_neurips_2021/","section":"publication","summary":"We provide strong empirical evidence to determine the practical validity of the linear approximation of neural networks for different learning tasks. Specifically, we discover that, in contrast to what was previously observed, neural networks do not always perform better than their kernel approximations, and reveal that their performance gap heavily depends on architecture, number of samples and training task.","tags":[],"title":"What can linearized neural networks actually say about generalization?","type":"publication"},{"authors":["G. Ortiz-Jimenez","I.F. Salazar-Reque","A. Modas","S.M. Moosavi-Dezfooli","P. Frossard"],"categories":[],"content":"","date":1620229949,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620229949,"objectID":"22cbf87380dc5cc2be55d2b4751dc713","permalink":"https://gortizji.github.io/publication/robustml_iclr_2021/","publishdate":"2021-05-03T12:52:29+02:00","relpermalink":"/publication/robustml_iclr_2021/","section":"publication","summary":"The underspecification of most machine learning pipelines means that we cannot rely solely on validation performance to assess the robustness of deep learning systems to naturally occurring distribution shifts. Instead, making sure that a neural network can generalize across a large number of different situations requires to understand the specific way in which it solves a task. In this work, we propose to study this problem from a geometric perspective with the aim to understand two key characteristics of neural network solutions in underspecified settings: how is the geometry of the learned function related to the data representation? And, are deep networks always biased towards simpler solutions, as conjectured in recent literature? We show that the way neural networks handle the underspecification of these problems is highly dependent on the data representation, affecting both the geometry and the complexity of the learned predictors. Our results highlight that understanding the architectural inductive bias in deep learning is fundamental to address the fairness, robustness, and generalization of these systems.","tags":[],"title":"A neural anisotropic view of underspecification in deep learning","type":"publication"},{"authors":["G. Ortiz-Jimenez*","A. Modas*","S.M. Moosavi-Dezfooli","P. Frossard"],"categories":[],"content":"","date":1607336444,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607336444,"objectID":"77e216b88945af70ea1c4874f98bb56d","permalink":"https://gortizji.github.io/publication/neural-anisotropy/","publishdate":"2020-06-18T12:20:44+02:00","relpermalink":"/publication/neural-anisotropy/","section":"publication","summary":"We analyze the role of the network architecture in shaping the inductive bias of deep classifiers. Specifically, we introduce the neural anisotropy directions (NADs) of a network as the vectors that encapsulate the directional inductive bias of an architecture. These vectors, encode the preference of a network to separate the input data based on some particular features.","tags":[],"title":"Neural Anisotropy Directions","type":"publication"},{"authors":["G. Ortiz-Jimenez*","A. Modas*","S.M. Moosavi-Dezfooli","P. Frossard"],"categories":[],"content":"","date":1607273415,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607273415,"objectID":"0ba3c4ef9871f40dd8928f4db9758e10","permalink":"https://gortizji.github.io/publication/hold_me_tight/","publishdate":"2020-02-15T17:50:15+01:00","relpermalink":"/publication/hold_me_tight/","section":"publication","summary":"Important insights towards the explainability of neural networks reside in the characteristics of their decision boundaries. In this work, we borrow tools from the field of adversarial robustness, and propose a new perspective that relates dataset features to the distance of samples to the decision boundary. Specifically, we rigorously confirm that neural networks exhibit a high invariance to non-discriminative features, and show that very small perturbations of the training samples in certain directions can lead to sudden invariances in the orthogonal ones.","tags":[],"title":"Hold me tight! Influence of discriminative features on deep network boundaries","type":"publication"},{"authors":["G. Ortiz-Jimenez","A. Modas","S.M. Moosavi-Dezfooli","P. Frossard"],"categories":[],"content":"","date":1603264842,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603264842,"objectID":"9d76ef7e9117025755b8f77cbd164731","permalink":"https://gortizji.github.io/publication/proc_ieee2020/","publishdate":"2020-10-21T09:20:42+02:00","relpermalink":"/publication/proc_ieee2020/","section":"publication","summary":" In this article, we provide an in-depth review of the field of adversarial robustness in deep learning, and give a self-contained introduction to its main notions. But, in contrast to the mainstream pessimistic perspective of adversarial robustness, we focus on the main positive aspects that it entails.","tags":[],"title":"Optimism in the Face of Adversity: Understanding and Improving Deep Learning through Adversarial Robustness","type":"publication"},{"authors":["G. Ortiz-Jimenez*","A. Modas*","S.M. Moosavi-Dezfooli","P. Frossard"],"categories":[],"content":"","date":1593964349,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593964349,"objectID":"6fe44521c04ac9174fb7ca90332d10f2","permalink":"https://gortizji.github.io/publication/udl_2020/","publishdate":"2020-07-05T17:52:29+02:00","relpermalink":"/publication/udl_2020/","section":"publication","summary":"In this work, we identify the subspace of features used by CNNs to classify large-scale vision benchmarks, and reveal some intriguing aspects of their robustness to distributions shift. Specifically,  we show that the existence of redundant features on a dataset can harm the networks' robustness to distribution shifts.","tags":[],"title":"Redundant features can hurt robustness to distribution shift","type":"publication"},{"authors":["C. Vignac","G. Ortiz-Jimenez","P. Frossard"],"categories":[],"content":"","date":1588344413,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588344413,"objectID":"932d80bfade91aaedb67ebf020db13a3","permalink":"https://gortizji.github.io/publication/icassp_gnn/","publishdate":"2019-10-30T15:46:53+02:00","relpermalink":"/publication/icassp_gnn/","section":"publication","summary":"In this paper, we show empirically that in settings with fewer features and more training data, more complex graph networks significantly outperform simpler architectures, and propose a few insights towards to the proper choice of graph neural networks architectures.","tags":[],"title":"On the Choice of Graph Neural Network Architectures","type":"publication"},{"authors":["G. Ortiz-Jimenez","M. El Gheche","E. Simou","H.P. Maretic","P. Frossard"],"categories":[],"content":"","date":1588340813,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588340813,"objectID":"84fde76052ac2b13581d0ec6fc59a84a","permalink":"https://gortizji.github.io/publication/icassp_cdot/","publishdate":"2019-10-30T15:46:53+02:00","relpermalink":"/publication/icassp_cdot/","section":"publication","summary":"In this paper, we devise a general forward-backward splitting algorithm based on Bregman distances for solving a wide range of optimization problems involving a differentiable function with Lipschitz-continuous gradient and a doubly stochastic constraint.","tags":[],"title":"Forward-Backward Splitting for Optimal Transport based Problems","type":"publication"},{"authors":["G. Ortiz-Jimenez","M. El Gheche","E. Simou","H.P. Maretic","P. Frossard"],"categories":[],"content":"","date":1576244408,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576244408,"objectID":"3f1262f03a27271783dd5d00df8903df","permalink":"https://gortizji.github.io/publication/otml_2019/","publishdate":"2019-09-13T15:40:08+02:00","relpermalink":"/publication/otml_2019/","section":"publication","summary":"In this work, we propose to exploit the geometry-awareness of optimal transport theory for the resolution of continuous domain adaptation problems. In particular, we address the scenario in which the target domain is continually, albeit slowly, evolving, and in which, at different time frames, we are given a batch of test data to classify.","tags":[],"title":"CDOT: Continuous Domain Adaptation using Optimal Transport","type":"publication"},{"authors":["G. Ortiz-Jimenez","M. Coutino","S. Chepuri","G. Leus"],"categories":[],"content":"","date":1557409208,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557409208,"objectID":"a14f06c180d98b16ca622363cb904788","permalink":"https://gortizji.github.io/publication/tsp_2019/","publishdate":"2019-06-09T15:40:08+02:00","relpermalink":"/publication/tsp_2019/","section":"publication","summary":"We consider the problem of designing sparse sampling strategies for multidomain signals, which can be represented using tensors that admit a known multilinear decomposition. We leverage the multidomain structure of tensor signals and propose to acquire samples using a Kronecker-structured sensing function, thereby circumventing the curse of dimensionality.","tags":[],"title":"Sparse Sampling For Inverse Problems with Tensors","type":"publication"},{"authors":["G. Ortiz-Jimenez","M. Coutino","S. Chepuri","G. Leus"],"categories":[],"content":"","date":1541771213,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541771213,"objectID":"9fec211d5264b4503f5128c678733590","permalink":"https://gortizji.github.io/publication/globalsip_2018/","publishdate":"2019-06-09T15:46:53+02:00","relpermalink":"/publication/globalsip_2018/","section":"publication","summary":"In this paper, we consider the problem of subsampling and reconstruction of signals that reside on the vertices of a product graph, such as sensor network time series, genomic signals, or product ratings in a social network.","tags":[],"title":"Sampling and Reconstruction of Signals on Product Graphs","type":"publication"},{"authors":["G. Ortiz-Jimenez"],"categories":[],"content":"","date":1535587200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535587200,"objectID":"8baeac4ed76864f29d8ef21d0df60505","permalink":"https://gortizji.github.io/publication/msc_thesis/","publishdate":"2018-08-30T00:00:00Z","relpermalink":"/publication/msc_thesis/","section":"publication","summary":"In this era of data deluge, we are overwhelmed with massive volumes of extremely complex datasets. Data generated today is complex because it lacks a clear geometric structure, comes in great volumes, and it often contains information from multiple domains. In this thesis, we address these issues and propose two theoretical frameworks to handle such multidomain dataset.","tags":[],"title":"Multidomain Graph Signal Processing","type":"publication"},{"authors":["G. Ortiz-Jimenez","F. Garcia-Rial","L. A. Ubeda-Medina","R. Pages","N. Garcia","J. Grajal"],"categories":[],"content":"","date":1499608013,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1499608013,"objectID":"36fdb3fcd1df4e4fe19163eb00a22805","permalink":"https://gortizji.github.io/publication/thz_2018/","publishdate":"2017-07-09T15:46:53+02:00","relpermalink":"/publication/thz_2018/","section":"publication","summary":"We present a simulation framework for a 3-D high-resolution imaging radar at 300 GHz with mechanical scanning. This tool allows us to reproduce the imaging capabilities of the radar in different setups and with different targets. The simulations are based on a ray-tracing approximation combined with a bidirectional reflectance distribution function (BRDF) model for the scattering of rough surfaces. Moreover, we present a novel approach to estimate the scattering parameters of the BRDF model for different types of targets from the combination of the radar data and information obtained from an infrared structure light sensor. This new framework will serve as a baseline for the design of future radar multistatic configurations and to generate synthetic data to train automatic target recognition algorithms.","tags":[],"title":"Simulation Framework for a 3-D High-Resolution Imaging Radar at 300 GHz with a Scattering Model Based on Rendering Techniques","type":"publication"}]