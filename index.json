[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a PhD student at EPFL working under the supervision of Pascal Frossard. My research focuses on understanding and enhancing AI systems by studying how weight space and function space are related in deep learning. I am particularly interested in making large pre-trained models more reliable and trustworthy.\nLast summer, I was a research intern at Google Research in Zurich, where I worked with Efi Kokiopoulou and Rodolphe Jenatton on scalable solutions for label noise. I have also visited Philip Torr\u0026rsquo;s lab at the University of Oxford as part of the ELLIS PhD Program, where I conducted research on the robustness of neural networks.\nBefore starting my PhD, I lived in The Netherlands where I worked on sampling theory for tensors and graphs at TU Delft. I also spent some great time in Hamburg at Philips Research working on self-supervised deep learning for medical imaging. Some time ago, I was a young undergraduate student at Universidad Politecnica de Madrid.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://gortizji.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a PhD student at EPFL working under the supervision of Pascal Frossard. My research focuses on understanding and enhancing AI systems by studying how weight space and function space are related in deep learning. I am particularly interested in making large pre-trained models more reliable and trustworthy.\nLast summer, I was a research intern at Google Research in Zurich, where I worked with Efi Kokiopoulou and Rodolphe Jenatton on scalable solutions for label noise.","tags":null,"title":"Guillermo Ortiz-Jiménez","type":"authors"},{"authors":["G. Ortiz-Jimenez*","A. Favero*","P. Frossard"],"categories":[],"content":"","date":1684837364,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684837364,"objectID":"02eeeceb617094fc254ce95bc3f5c467","permalink":"https://gortizji.github.io/publication/task_ntk/","publishdate":"2023-05-23T12:20:44+02:00","relpermalink":"/publication/task_ntk/","section":"publication","summary":"We present a comprehensive study of task arithmetic in vision-language models and show that weight disentanglement is the crucial factor that makes it effective. Notably, we show that fine-tuning models in their tangent space by linearizing them amplifies weight disentanglement. This leads to substantial performance improvements across multiple task arithmetic benchmarks and diverse models.","tags":[],"title":"Task arithmetic in the tangent space: Improved editing of pre-trained models","type":"publication"},{"authors":["G. Ortiz-Jimenez*","M. Collier*","A. Nawalgaria","A. D'Amour","J. Berent","R. Jenatton","E. Kokiopoulou"],"categories":[],"content":"","date":1678272749,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678272749,"objectID":"0c78978c25057870bce7299f9b424f6a","permalink":"https://gortizji.github.io/publication/pi_google/","publishdate":"2023-03-08T12:52:29+02:00","relpermalink":"/publication/pi_google/","section":"publication","summary":"We present a large-scale benchmark of label noise methods that leverage privileged information and analyze why and when they work.","tags":[],"title":"When does privileged information explain away label noise?","type":"publication"},{"authors":["G. Ortiz-Jimenez*","P. de Jorge*","A. Sanyal","A. Bibi","P. Dokania","P. Frossard","G. Rogez","P. Torr"],"categories":[],"content":"","date":1656325364,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656325364,"objectID":"2387ab5d9c317e1f80272853d5f48ea6","permalink":"https://gortizji.github.io/publication/co_oxford/","publishdate":"2022-06-27T12:20:44+02:00","relpermalink":"/publication/co_oxford/","section":"publication","summary":"We find that the interplay between the structure of the data and the dynamics of AT plays a fundamental role in CO. Specifically, through active interventions on typical datasets of natural images, we establish a causal link between the structure of the data and the onset of CO in single-step AT methods.","tags":[],"title":"Catastrophic overfitting can be induced with discriminative non-robust features","type":"publication"},{"authors":["A. Modas*","R. Rade*","G. Ortiz-Jimenez","S.M. Moosavi-Dezfooli","P. Frossard"],"categories":[],"content":"","date":1656325364,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656325364,"objectID":"e4483ff10d8fccc5b415a0e1240825d6","permalink":"https://gortizji.github.io/publication/prime_2021/","publishdate":"2021-12-27T12:20:44+02:00","relpermalink":"/publication/prime_2021/","section":"publication","summary":"We propose PRIME, a general data augmentation scheme that consists of simple families of max-entropy image transformations. We show that PRIME outperforms the prior art for corruption robustness, while its simplicity and plug-and-play nature enables it to be combined with other methods to further boost their robustness.","tags":[],"title":"PRIME: A few primitives can boost robustness to common corruptions","type":"publication"},{"authors":["G. Yüce*","G. Ortiz-Jimenez*","B. Besbinar","P. Frossard"],"categories":[],"content":"","date":1638787949,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638787949,"objectID":"a3dc047ed7639e830bc921aa42c364bd","permalink":"https://gortizji.github.io/publication/inr_dictionaries/","publishdate":"2021-12-06T12:52:29+02:00","relpermalink":"/publication/inr_dictionaries/","section":"publication","summary":"We show that most INR families are analogous to structured signal dictionaries whose atoms are integer harmonics of the set of initial mapping frequencies whose inductive bias is akin to that of signal dictionary formed by the eigenfunctions of the NTK at initialization. We reveal that meta-learning has a reshaping effect of the NTK analogous to dictionary learning, building dictionary atoms as a combination of the examples seen during meta-training.","tags":[],"title":"A structured dictionary perspective on implicit neural representations","type":"publication"},{"authors":["G. Ortiz-Jimenez","S.M. Moosavi-Dezfooli","P. Frossard"],"categories":[],"content":"","date":1632824564,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632824564,"objectID":"2149b760f6891397b6b45558210827f9","permalink":"https://gortizji.github.io/publication/linearized_nets_neurips_2021/","publishdate":"2021-06-12T12:20:44+02:00","relpermalink":"/publication/linearized_nets_neurips_2021/","section":"publication","summary":"We provide strong empirical evidence to determine the practical validity of the linear approximation of neural networks for different learning tasks. Specifically, we discover that, in contrast to what was previously observed, neural networks do not always perform better than their kernel approximations, and reveal that their performance gap heavily depends on architecture, number of samples and training task.","tags":[],"title":"What can linearized neural networks actually say about generalization?","type":"publication"},{"authors":["G. Ortiz-Jimenez","I.F. Salazar-Reque","A. Modas","S.M. Moosavi-Dezfooli","P. Frossard"],"categories":[],"content":"","date":1620229949,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620229949,"objectID":"22cbf87380dc5cc2be55d2b4751dc713","permalink":"https://gortizji.github.io/publication/robustml_iclr_2021/","publishdate":"2021-05-03T12:52:29+02:00","relpermalink":"/publication/robustml_iclr_2021/","section":"publication","summary":"The underspecification of most machine learning pipelines means that we cannot rely solely on validation performance to assess the robustness of deep learning systems to naturally occurring distribution shifts. Instead, making sure that a neural network can generalize across a large number of different situations requires to understand the specific way in which it solves a task. In this work, we propose to study this problem from a geometric perspective with the aim to understand two key characteristics of neural network solutions in underspecified settings: how is the geometry of the learned function related to the data representation? And, are deep networks always biased towards simpler solutions, as conjectured in recent literature? We show that the way neural networks handle the underspecification of these problems is highly dependent on the data representation, affecting both the geometry and the complexity of the learned predictors. Our results highlight that understanding the architectural inductive bias in deep learning is fundamental to address the fairness, robustness, and generalization of these systems.","tags":[],"title":"A neural anisotropic view of underspecification in deep learning","type":"publication"},{"authors":["G. Ortiz-Jimenez*","A. Modas*","S.M. Moosavi-Dezfooli","P. Frossard"],"categories":[],"content":"","date":1607336444,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607336444,"objectID":"77e216b88945af70ea1c4874f98bb56d","permalink":"https://gortizji.github.io/publication/neural-anisotropy/","publishdate":"2020-06-18T12:20:44+02:00","relpermalink":"/publication/neural-anisotropy/","section":"publication","summary":"We analyze the role of the network architecture in shaping the inductive bias of deep classifiers. Specifically, we introduce the neural anisotropy directions (NADs) of a network as the vectors that encapsulate the directional inductive bias of an architecture. These vectors, encode the preference of a network to separate the input data based on some particular features.","tags":[],"title":"Neural Anisotropy Directions","type":"publication"},{"authors":["G. Ortiz-Jimenez*","A. Modas*","S.M. Moosavi-Dezfooli","P. Frossard"],"categories":[],"content":"","date":1607273415,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607273415,"objectID":"0ba3c4ef9871f40dd8928f4db9758e10","permalink":"https://gortizji.github.io/publication/hold_me_tight/","publishdate":"2020-02-15T17:50:15+01:00","relpermalink":"/publication/hold_me_tight/","section":"publication","summary":"Important insights towards the explainability of neural networks reside in the characteristics of their decision boundaries. In this work, we borrow tools from the field of adversarial robustness, and propose a new perspective that relates dataset features to the distance of samples to the decision boundary. Specifically, we rigorously confirm that neural networks exhibit a high invariance to non-discriminative features, and show that very small perturbations of the training samples in certain directions can lead to sudden invariances in the orthogonal ones.","tags":[],"title":"Hold me tight! Influence of discriminative features on deep network boundaries","type":"publication"},{"authors":["G. Ortiz-Jimenez","A. Modas","S.M. Moosavi-Dezfooli","P. Frossard"],"categories":[],"content":"","date":1603264842,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603264842,"objectID":"9d76ef7e9117025755b8f77cbd164731","permalink":"https://gortizji.github.io/publication/proc_ieee2020/","publishdate":"2020-10-21T09:20:42+02:00","relpermalink":"/publication/proc_ieee2020/","section":"publication","summary":" In this article, we provide an in-depth review of the field of adversarial robustness in deep learning, and give a self-contained introduction to its main notions. But, in contrast to the mainstream pessimistic perspective of adversarial robustness, we focus on the main positive aspects that it entails.","tags":[],"title":"Optimism in the Face of Adversity: Understanding and Improving Deep Learning through Adversarial Robustness","type":"publication"},{"authors":["G. Ortiz-Jimenez*","A. Modas*","S.M. Moosavi-Dezfooli","P. Frossard"],"categories":[],"content":"","date":1593964349,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593964349,"objectID":"6fe44521c04ac9174fb7ca90332d10f2","permalink":"https://gortizji.github.io/publication/udl_2020/","publishdate":"2020-07-05T17:52:29+02:00","relpermalink":"/publication/udl_2020/","section":"publication","summary":"In this work, we identify the subspace of features used by CNNs to classify large-scale vision benchmarks, and reveal some intriguing aspects of their robustness to distributions shift. Specifically,  we show that the existence of redundant features on a dataset can harm the networks' robustness to distribution shifts.","tags":[],"title":"Redundant features can hurt robustness to distribution shift","type":"publication"},{"authors":["C. Vignac","G. Ortiz-Jimenez","P. Frossard"],"categories":[],"content":"","date":1588344413,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588344413,"objectID":"932d80bfade91aaedb67ebf020db13a3","permalink":"https://gortizji.github.io/publication/icassp_gnn/","publishdate":"2019-10-30T15:46:53+02:00","relpermalink":"/publication/icassp_gnn/","section":"publication","summary":"In this paper, we show empirically that in settings with fewer features and more training data, more complex graph networks significantly outperform simpler architectures, and propose a few insights towards to the proper choice of graph neural networks architectures.","tags":[],"title":"On the Choice of Graph Neural Network Architectures","type":"publication"},{"authors":["G. Ortiz-Jimenez","M. El Gheche","E. Simou","H.P. Maretic","P. Frossard"],"categories":[],"content":"","date":1588340813,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588340813,"objectID":"84fde76052ac2b13581d0ec6fc59a84a","permalink":"https://gortizji.github.io/publication/icassp_cdot/","publishdate":"2019-10-30T15:46:53+02:00","relpermalink":"/publication/icassp_cdot/","section":"publication","summary":"In this paper, we devise a general forward-backward splitting algorithm based on Bregman distances for solving a wide range of optimization problems involving a differentiable function with Lipschitz-continuous gradient and a doubly stochastic constraint.","tags":[],"title":"Forward-Backward Splitting for Optimal Transport based Problems","type":"publication"},{"authors":["G. Ortiz-Jimenez","M. Coutino","S. Chepuri","G. Leus"],"categories":[],"content":"","date":1557409208,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557409208,"objectID":"a14f06c180d98b16ca622363cb904788","permalink":"https://gortizji.github.io/publication/tsp_2019/","publishdate":"2019-06-09T15:40:08+02:00","relpermalink":"/publication/tsp_2019/","section":"publication","summary":"We consider the problem of designing sparse sampling strategies for multidomain signals, which can be represented using tensors that admit a known multilinear decomposition. We leverage the multidomain structure of tensor signals and propose to acquire samples using a Kronecker-structured sensing function, thereby circumventing the curse of dimensionality.","tags":[],"title":"Sparse Sampling For Inverse Problems with Tensors","type":"publication"},{"authors":["G. Ortiz-Jimenez","M. Coutino","S. Chepuri","G. Leus"],"categories":[],"content":"","date":1541771213,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541771213,"objectID":"9fec211d5264b4503f5128c678733590","permalink":"https://gortizji.github.io/publication/globalsip_2018/","publishdate":"2019-06-09T15:46:53+02:00","relpermalink":"/publication/globalsip_2018/","section":"publication","summary":"In this paper, we consider the problem of subsampling and reconstruction of signals that reside on the vertices of a product graph, such as sensor network time series, genomic signals, or product ratings in a social network.","tags":[],"title":"Sampling and Reconstruction of Signals on Product Graphs","type":"publication"},{"authors":["G. Ortiz-Jimenez"],"categories":[],"content":"","date":1535587200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535587200,"objectID":"8baeac4ed76864f29d8ef21d0df60505","permalink":"https://gortizji.github.io/publication/msc_thesis/","publishdate":"2018-08-30T00:00:00Z","relpermalink":"/publication/msc_thesis/","section":"publication","summary":"In this era of data deluge, we are overwhelmed with massive volumes of extremely complex datasets. Data generated today is complex because it lacks a clear geometric structure, comes in great volumes, and it often contains information from multiple domains. In this thesis, we address these issues and propose two theoretical frameworks to handle such multidomain dataset.","tags":[],"title":"Multidomain Graph Signal Processing","type":"publication"},{"authors":["G. Ortiz-Jimenez","F. Garcia-Rial","L. A. Ubeda-Medina","R. Pages","N. Garcia","J. Grajal"],"categories":[],"content":"","date":1499608013,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1499608013,"objectID":"36fdb3fcd1df4e4fe19163eb00a22805","permalink":"https://gortizji.github.io/publication/thz_2018/","publishdate":"2017-07-09T15:46:53+02:00","relpermalink":"/publication/thz_2018/","section":"publication","summary":"We present a simulation framework for a 3-D high-resolution imaging radar at 300 GHz with mechanical scanning. This tool allows us to reproduce the imaging capabilities of the radar in different setups and with different targets. The simulations are based on a ray-tracing approximation combined with a bidirectional reflectance distribution function (BRDF) model for the scattering of rough surfaces. Moreover, we present a novel approach to estimate the scattering parameters of the BRDF model for different types of targets from the combination of the radar data and information obtained from an infrared structure light sensor. This new framework will serve as a baseline for the design of future radar multistatic configurations and to generate synthetic data to train automatic target recognition algorithms.","tags":[],"title":"Simulation Framework for a 3-D High-Resolution Imaging Radar at 300 GHz with a Scattering Model Based on Rendering Techniques","type":"publication"}]